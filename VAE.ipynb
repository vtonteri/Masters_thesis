{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import scipy.signal\n",
    "import pydub\n",
    "import librosa\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from IPython.display import Audio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.utils.data import random_split\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import tools\n",
    "import tests\n",
    "import time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80__04_10_12_adricristuy.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10__10_07_13_marDeOnza_Sale.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14__10_07_13_piraCies_Espera.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15__10_07_13_radaUno_Pasa.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6__10_07_13_marDeCangas_Entra.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       relative_path  class_id\n",
       "0       80__04_10_12_adricristuy.wav         0\n",
       "1    10__10_07_13_marDeOnza_Sale.wav         1\n",
       "2   14__10_07_13_piraCies_Espera.wav         1\n",
       "3      15__10_07_13_radaUno_Pasa.wav         2\n",
       "4  6__10_07_13_marDeCangas_Entra.wav         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNOTATIONS_FILE = r'C:\\shipsEar_AUDIOS\\shipsEar.xlsx'\n",
    "AUDIO_DIR = r'C:\\shipsEar_AUDIOS'\n",
    "SAMPLE_RATE = 52734\n",
    "NUM_SAMPLES = 16000\n",
    "\n",
    "metadata_file = ANNOTATIONS_FILE\n",
    "df = pd.read_excel(metadata_file)\n",
    "df[\"relative_path\"] = df[\"Filename\"]\n",
    "type_mapping = {type_name: class_id for class_id, type_name in enumerate(df[\"Type\"].unique())}\n",
    "df[\"class_id\"] = df[\"Type\"].map(type_mapping)\n",
    "df = df[[\"relative_path\", \"class_id\"]]\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtilization():\n",
    "\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rechannel(audio, new_channel):\n",
    "\n",
    "        # Convert audio to the desired number of channels\n",
    "        sig, sr = audio\n",
    "\n",
    "        if sig.shape[0] == new_channel:\n",
    "            return audio\n",
    "        \n",
    "        if new_channel == 1:\n",
    "            resig = sig[:1, :]\n",
    "        \n",
    "        else:\n",
    "            resig = torch.cat([sig,sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(audio, max_ms):\n",
    "        sig, sr = audio\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "\n",
    "        if sig_len > max_len:\n",
    "            sig = sig[:,:max_len]\n",
    "        \n",
    "        elif sig_len < max_len:\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            pad_begin = torch.zeros((num_rows,pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows,pad_end_len))\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_shift(audio, shift_limit):\n",
    "        sig, sr = audio\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_gram(audio, n_mels = 64, n_fft = 1024, hop_len = None):\n",
    "        sig, sr = audio\n",
    "        top_db = 100\n",
    "\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        # Conversion to decibels:\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "\n",
    "        return (spec)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augmen(spec, max_mask_pct = 0.1, n_freq_masks = 1, n_time_masks = 1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_spectrogram(spec, max_len):\n",
    "        n_channels, n_mels, n_steps = spec.shape\n",
    "        if n_steps < max_len:\n",
    "            pad_amount = max_len - n_steps\n",
    "            pad_spec = torch.nn.functional.pad(spec, (0, pad_amount), \"constant\", 0)\n",
    "        else:\n",
    "            pad_spec = spec[:, :, :max_len]\n",
    "        return pad_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 52734\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #audio_file = self.data_path + self.df.loc[idx, \"relative_path\"]\n",
    "        #print(f\"this is self.path: {self.data_path}\")\n",
    "        #rel_path = self.df.loc[idx, \"relative_path\"]\n",
    "\n",
    "        audio_file = os.path.join(self.data_path, self.df.loc[idx, \"relative_path\"])\n",
    "\n",
    "        class_id = self.df.loc[idx, \"class_id\"]\n",
    "\n",
    "\n",
    "        audio = AudioUtilization.open(audio_file)\n",
    "        re_channel = AudioUtilization.rechannel(audio, self.channel)\n",
    "        dur_audio = AudioUtilization.pad_trunc(re_channel, self.duration)\n",
    "        shift_audio = AudioUtilization.time_shift(dur_audio, self.shift_pct)\n",
    "        spectrogram = AudioUtilization.spectro_gram(shift_audio, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_spectrogram = AudioUtilization.spectro_augmen(spectrogram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return aug_spectrogram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset_longer(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 10000\n",
    "        self.sr = 52734\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #audio_file = self.data_path + self.df.loc[idx, \"relative_path\"]\n",
    "        #print(f\"this is self.path: {self.data_path}\")\n",
    "        #rel_path = self.df.loc[idx, \"relative_path\"]\n",
    "\n",
    "        audio_file = os.path.join(self.data_path, self.df.loc[idx, \"relative_path\"])\n",
    "\n",
    "        class_id = self.df.loc[idx, \"class_id\"]\n",
    "\n",
    "\n",
    "        audio = AudioUtilization.open(audio_file)\n",
    "        re_channel = AudioUtilization.rechannel(audio, self.channel)\n",
    "        dur_audio = AudioUtilization.pad_trunc(re_channel, self.duration)\n",
    "        shift_audio = AudioUtilization.time_shift(dur_audio, self.shift_pct)\n",
    "        spectrogram = AudioUtilization.spectro_gram(shift_audio, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_spectrogram = AudioUtilization.spectro_augmen(spectrogram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return aug_spectrogram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network classifier\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def custom_collate_fn(batch):\n",
    "        max_len = max([item[0].shape[2] for item in batch])\n",
    "        specs = [AudioUtilization.pad_spectrogram(item[0], max_len) for item in batch]\n",
    "        labels = torch.tensor([item[1] for item in batch])\n",
    "        return torch.stack(specs), labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing dataloader for shorter spectrograms (4 s)\n",
    "\n",
    "data_path = r\"C:\\shipsEar_AUDIOS\"\n",
    "\n",
    "myds = AudioDataset(df, data_path)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=AudioClassifier.custom_collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=AudioClassifier.custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing dataloader for longer spectrograms (10 s)\n",
    "\n",
    "data_path = r\"C:\\shipsEar_AUDIOS\"\n",
    "\n",
    "myds = AudioDataset_longer(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds_long, val_ds_long = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl_long = torch.utils.data.DataLoader(train_ds_long, batch_size=16, shuffle=True, collate_fn=AudioClassifier.custom_collate_fn)\n",
    "val_dl_long = torch.utils.data.DataLoader(val_ds_long, batch_size=16, shuffle=False, collate_fn=AudioClassifier.custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model hyperparameters\n",
    "LR = 0.001\n",
    "PATIENCE = 2\n",
    "IMAGE_SIZE = \n",
    "CHANNELS = 2\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 13\n",
    "EPOCHS = 100\n",
    "SHAPE_BEFORE_FLATTENING = (128, IMAGE_SIZE // 8, IMAGE_SIZE // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_gaussian_kl_loss(mu, logvar):\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return KLD.mean()\n",
    "\n",
    "def reconstruction_loss(x_reconstructed, x):\n",
    "    bce_loss = nn.BCELoss()\n",
    "    return bce_loss(x_reconstructed, x)\n",
    "\n",
    "def vae_loss(y_pred, y_true):\n",
    "    mu, logvar, recon_x = y_pred\n",
    "    recon_loss = reconstruction_loss(recon_x, y_true)\n",
    "    kld_loss = vae_gaussian_kl_loss(mu, logvar)\n",
    "    return 500 * recon_loss + kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pyimagesearch.com/2023/10/02/a-deep-dive-into-variational-autoencoders-with-pytorch/\n",
    "\n",
    "# VARIATIONAL AUTOENCODER\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        # get the shape of the tensor for the mean and log variance\n",
    "        batch, dim = z_mean.shape\n",
    "        # generate a normal random tensor (epsilon) with the same shape as z_mean\n",
    "        # this tensor will be used for reparameterization trick\n",
    "        epsilon = Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
    "        # apply the reparameterization trick to generate the samples in the\n",
    "        # latent space\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_size, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # define the convolutional layers for downsampling and feature\n",
    "        # extraction\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
    "        # define a flatten layer to flatten the tensor before feeding it into\n",
    "        # the fully connected layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        # define fully connected layers to transform the tensor into the desired\n",
    "        # embedding dimensions\n",
    "        self.fc_mean = nn.Linear(\n",
    "            128 * (image_size // 8) * (image_size // 8), embedding_dim\n",
    "        )\n",
    "        self.fc_log_var = nn.Linear(\n",
    "            128 * (image_size // 8) * (image_size // 8), embedding_dim\n",
    "        )\n",
    "        # initialize the sampling layer\n",
    "        self.sampling = Sampling()\n",
    "    def forward(self, x):\n",
    "        # apply convolutional layers with relu activation function\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # flatten the tensor\n",
    "        x = self.flatten(x)\n",
    "        # get the mean and log variance of the latent space distribution\n",
    "        z_mean = self.fc_mean(x)\n",
    "        z_log_var = self.fc_log_var(x)\n",
    "        # sample a latent vector using the reparameterization trick\n",
    "        z = self.sampling(z_mean, z_log_var)\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, shape_before_flattening):\n",
    "        super(Decoder, self).__init__()\n",
    "        # define a fully connected layer to transform the latent vector back to\n",
    "        # the shape before flattening\n",
    "        self.fc = nn.Linear(\n",
    "            embedding_dim,\n",
    "            shape_before_flattening[0]\n",
    "            * shape_before_flattening[1]\n",
    "            * shape_before_flattening[2],\n",
    "        )\n",
    "        # define a reshape function to reshape the tensor back to its original\n",
    "        # shape\n",
    "        self.reshape = lambda x: x.view(-1, *shape_before_flattening)\n",
    "        # define the transposed convolutional layers for the decoder to upsample\n",
    "        # and generate the reconstructed image\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            128, 64, 3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            64, 32, 3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            32, 1, 3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # pass the latent vector through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # reshape the tensor\n",
    "        x = self.reshape(x)\n",
    "        # apply transposed convolutional layers with relu activation function\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        # apply the final transposed convolutional layer with a sigmoid\n",
    "        # activation to generate the final output\n",
    "        x = torch.sigmoid(self.deconv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        # initialize the encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x):\n",
    "        # pass the input through the encoder to get the latent vector\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        # pass the latent vector through the decoder to get the reconstructed\n",
    "        # image\n",
    "        reconstruction = self.decoder(z)\n",
    "        # return the mean, log variance and the reconstructed image\n",
    "        return z_mean, z_log_var, reconstruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
